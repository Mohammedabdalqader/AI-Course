{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal character-level LSTM model. Written by Ngoc Quan Pham\n",
    "Code structure borrowed from the Vanilla RNN model from Andreij Karparthy @karparthy.\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from random import uniform\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "data = open('data/input.txt', 'r').read() # should be simple plain text file\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "std = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since numpy doesn't have a function for sigmoid\n",
    "# We implement it manually here\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# The derivative of the sigmoid function\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# The derivative of the tanh function\n",
    "def dtanh(x):\n",
    "    return 1 - x*x\n",
    "\n",
    "\n",
    "# The numerically stable softmax implementation\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "emb_size = 4\n",
    "hidden_size = 32  # size of hidden layer of neurons\n",
    "seq_length = 64  # number of steps to unroll the RNN for\n",
    "learning_rate = 5e-2\n",
    "max_updates = 500000\n",
    "\n",
    "concat_size = emb_size + hidden_size # 36 ( A + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "# char embedding parameters\n",
    "Wex = np.random.randn(emb_size, vocab_size)*std # embedding layer (4,65)\n",
    "\n",
    "# LSTM parameters\n",
    "Wf = np.random.randn(hidden_size, concat_size) * std # forget gate (32,36)\n",
    "Wi = np.random.randn(hidden_size, concat_size) * std # input gate \n",
    "Wo = np.random.randn(hidden_size, concat_size) * std # output gate\n",
    "Wc = np.random.randn(hidden_size, concat_size) * std # c term\n",
    "\n",
    "bf = np.zeros((hidden_size, 1)) # forget bias\n",
    "bi = np.zeros((hidden_size, 1)) # input bias\n",
    "bo = np.zeros((hidden_size, 1)) # output bias\n",
    "bc = np.zeros((hidden_size, 1)) # memory bias\n",
    "\n",
    "# Output layer parameters\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output (65, 32) \n",
    "by = np.zeros((vocab_size, 1)) # output bias (65,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inputs, targets, memory):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    \n",
    "    # The LSTM is different than the simple RNN that it has two memory cells\n",
    "    # so here you need two different hidden layers\n",
    "    h_prev, c_prev = memory\n",
    "    \n",
    "    # Here you should allocate some variables to store the activations during forward\n",
    "    # One of them here is to store the hiddens and the cells\n",
    "    hs,cs,xs,wes,zs,fg,ig,cct,cs,og,logits,p,ys = {},{},{},{},{},{},{},{},{},{},{},{},{}\n",
    "\n",
    "\n",
    "    hs[-1] = np.copy(h_prev)\n",
    "    cs[-1] = np.copy(c_prev)\n",
    "\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "\n",
    "            \n",
    "        # convert word indices to word embeddings\n",
    "        wes[t] = np.dot(Wex, xs[t])\n",
    "        \n",
    "\n",
    "        # LSTM cell operation\n",
    "        # first concatenate the input and h\n",
    "        # This step is irregular (to save the amount of matrix multiplication we have to do)\n",
    "        # I will refer to this vector as [h X]\n",
    "        zs[t] = np.row_stack((hs[t-1], wes[t]))\n",
    "\n",
    "\n",
    "        # YOUR IMPLEMENTATION should begin from here\n",
    "\n",
    "        # compute the forget gate\n",
    "        fg[t] = sigmoid(np.dot(Wf,zs[t]) + bf)\n",
    "\n",
    "        # compute the input gate\n",
    "        ig[t] = sigmoid(np.dot(Wi,zs[t]) + bi)\n",
    "        \n",
    "        # output gate\n",
    "        og[t] = sigmoid(np.dot(Wo, zs[t]) + bo)\n",
    "\n",
    "        # compute the candidate memory\n",
    "        cct[t] = np.tanh(np.dot(Wc, zs[t]) + bc)\n",
    "\n",
    "        # new memory: applying forget gate on the previous memory\n",
    "        # and then adding the input gate on the candidate memory\n",
    "        cs[t] = fg[t] * cs[t-1] + ig[t] * cct[t]\n",
    "\n",
    "\n",
    "\n",
    "        # new hidden state for the LSTM\n",
    "        hs[t] = og[t] * np.tanh(cs[t])\n",
    "\n",
    "        # DONE LSTM\n",
    "        # output layer - softmax and cross-entropy loss\n",
    "        # unnormalized log probabilities for next chars\n",
    "\n",
    "        logits[t] = np.dot(Why, hs[t]) + by\n",
    "\n",
    "\n",
    "\n",
    "        # softmax for probabilities for next chars\n",
    "        p[t] = softmax(logits[t])\n",
    "        \n",
    "        \n",
    "\n",
    "        # cross-entropy loss\n",
    "        # cross entropy loss at time t:\n",
    "        # create an one hot vector for the label y\n",
    "        # and then cross-entropy (see the elman-rnn file for the hint)\n",
    "\n",
    "        ys[t] = np.zeros((vocab_size, 1))\n",
    "        ys[t][targets[t]] = 1\n",
    "        \n",
    "        loss_t = np.sum(-np.log(p[t]) * ys[t])\n",
    "\n",
    "        loss += loss_t\n",
    "\n",
    "    # define your activations\n",
    "    activations = (hs,cs,xs,wes,zs,fg,ig,cct,cs,og,logits,p,ys)\n",
    "    memory = (hs[len(inputs)-1], cs[len(inputs)-1])\n",
    "    #print(loss)\n",
    "    return loss, activations, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(activations, clipping=True):\n",
    "\n",
    "    # backward pass: compute gradients going backwards\n",
    "    # Here we allocate memory for the gradients\n",
    "    dWex, dWhy = np.zeros_like(Wex), np.zeros_like(Why)\n",
    "    dby = np.zeros_like(by)\n",
    "    dWf, dWi, dWc, dWo = np.zeros_like(Wf), np.zeros_like(Wi),np.zeros_like(Wc), np.zeros_like(Wo)\n",
    "    dbf, dbi, dbc, dbo = np.zeros_like(bf), np.zeros_like(bi),np.zeros_like(bc), np.zeros_like(bo)\n",
    "    \n",
    "    \n",
    "    hs, cs, xs, wes, zs, fg, ig, cct, cs, og, logits, p, ys = activations\n",
    "\n",
    "    # similar to the hidden states in the vanilla RNN\n",
    "    # We need to initialize the gradients for these variables\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    dcnext = np.zeros_like(cs[0])\n",
    "\n",
    "    # back propagation through time starts here\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # IMPLEMENT YOUR BACKPROP HERE\n",
    "        # refer to the file elman_rnn.py for more details\n",
    "        dlogits = p[t] - ys[t]\n",
    "        dWhy += np.dot(dlogits, hs[t].T)\n",
    "        dby += dlogits\n",
    "        \n",
    "        dh = np.dot(Why.T, dlogits) \n",
    "        dh += dhnext\n",
    "\n",
    "        dog = tanh(cs[t]) * dh\n",
    "        dWo += np.dot((dsigmoid(og[t])*dog), zs[t].T)\n",
    "        dbo += dsigmoid(og[t])*dog\n",
    "        #########################################\n",
    "        \n",
    "        #dcs = np.copy(dcnext)\n",
    "        dcs = og[t] * dh * dtanh(tanh(cs[t]))\n",
    "        dcs += dcnext\n",
    "        dcct = dcs*ig[t]\n",
    "        dWc += np.dot(dtanh(cct[t]) * dcct, zs[t].T)\n",
    "        dbc += dtanh(cct[t]) * dcct\n",
    "        \n",
    "        dig = dcs * cct[t]\n",
    "        dWi += np.dot(dsigmoid(ig[t]) * dig, zs[t].T)\n",
    "        dbi += dsigmoid(ig[t]) * dig\n",
    "        #################################\n",
    "        dfg = dcs * cs[t-1]\n",
    "        dWf += np.dot(dsigmoid(fg[t]) * dfg, zs[t].T)\n",
    "        dbf += dsigmoid(fg[t]) * dfg\n",
    "        \n",
    "        dzs = (np.dot(Wf.T, dsigmoid(fg[t]) * dfg)\n",
    "         + np.dot(Wi.T, dsigmoid(ig[t]) * dig)\n",
    "         + np.dot(Wc.T, dtanh(cct[t]) * dcct)\n",
    "         + np.dot(Wo.T, dsigmoid(og[t])*dog))\n",
    "        \n",
    "        dhnext = dzs[:hidden_size, :]\n",
    "        dwes = dzs[hidden_size:, :]\n",
    "        dWex += np.dot(dwes ,xs[t].T)\n",
    "        dcnext = fg[t] * dcs\n",
    "        assert(cct[t].shape==dcct.shape)\n",
    "        assert(cs[t].shape==dcs.shape)\n",
    "        assert(Wf.shape==dWf.shape)\n",
    "        assert(wes[t].shape==dwes.shape)\n",
    "        \n",
    "\n",
    "    if clipping:\n",
    "        # clip to mitigate exploding gradients\n",
    "        for dparam in [dWex, dWf, dWi, dWo, dWc, dbf, dbi, dbo, dbc, dWhy, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "    gradients = (dWex, dWf, dWi, dWo, dWc, dbf, dbi, dbo, dbc, dWhy, dby)\n",
    "    \n",
    "\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(memory, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "    h is memory state, seed_ix is seed letter for first time step   \n",
    "    \"\"\"\n",
    "    h, c = memory\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    generated_chars = []\n",
    "    for t in range(n):\n",
    "\n",
    "        wes = np.dot(Wex, x)\n",
    "        zs = np.row_stack((h, wes))\n",
    "        f_gate= sigmoid(np.dot(Wf,zs) + bf)\n",
    "        i_gate = sigmoid(np.dot(Wi,zs) + bi)\n",
    "        cct = np.tanh(np.dot(Wc, zs) + bc)\n",
    "        c = f_gate * c + i_gate * cct\n",
    "        ot = sigmoid(np.dot(Wo, zs) + bo)\n",
    "        h = ot * np.tanh(c)\n",
    "        out = np.dot(Why, h) + by\n",
    "        p= softmax(out)\n",
    "        \n",
    "        ix = np.random.multinomial(1, p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "\n",
    "        for j in range(len(ix)):\n",
    "            if ix[j] == 1:\n",
    "                index = j\n",
    "        x[index] = 1\n",
    "        generated_chars.append(index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return generated_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " zMa:VlCsEVKnk'mjYVZRYZwOPAifjl'IdPmFgiZj:hvjnmRHP?Nao!;XQ&W \n",
      "dvh HeRh!RC-HpQXapnpTL,SkPQdMt'E:,w\n",
      "HWm3bS;XdXnuAr,NlXBD&nA.DUKyzgd!PlbVYbP ?D3R:It:n.ke'GcKQ' ?CJgYOi:Hv?hh;3pikfbv.CqNSPck'iDGS\n",
      "q t?ScFTn \n",
      "----\n",
      "iter 0, loss: 267.160786\n",
      "iter 100, loss: 250.009909\n",
      "iter 200, loss: 227.045792\n",
      "iter 300, loss: 205.723770\n",
      "iter 400, loss: 186.307351\n",
      "iter 500, loss: 168.686889\n",
      "iter 600, loss: 152.715425\n",
      "iter 700, loss: 138.246680\n",
      "iter 800, loss: 125.143538\n",
      "iter 900, loss: 113.279352\n",
      "----\n",
      " irst CitiznnnBefore we proceed any furtper, hear me speak.\n",
      "\n",
      "Allll\n",
      "rs  ttCitizen:\n",
      "Befoceedany further, hear me speak.\n",
      "\n",
      "Alllll\n",
      "rsd sperr me proceed any further, hear me speak.\n",
      "\n",
      "Allll\n",
      "rd a' aroceed any f \n",
      "----\n",
      "iter 1000, loss: 102.538259\n",
      "iter 1100, loss: 92.814740\n",
      "iter 1200, loss: 84.012905\n",
      "iter 1300, loss: 76.045723\n",
      "iter 1400, loss: 68.834279\n",
      "iter 1500, loss: 62.307045\n",
      "iter 1600, loss: 56.399222\n",
      "iter 1700, loss: 51.052120\n",
      "iter 1800, loss: 46.212585\n",
      "iter 1900, loss: 41.832482\n",
      "----\n",
      " irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "Alllll\n",
      "red any further, hear me speak.\n",
      "\n",
      "Allll,ir tCitizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "Allllrir C Citizen:\n",
      "Before speak. \n",
      "----\n",
      "iter 2000, loss: 37.868226\n",
      "iter 2100, loss: 34.280355\n",
      "iter 2200, loss: 31.033144\n",
      "iter 2300, loss: 28.094253\n",
      "iter 2400, loss: 25.434407\n",
      "iter 2500, loss: 23.027109\n",
      "iter 2600, loss: 20.848372\n",
      "iter 2700, loss: 18.876490\n",
      "iter 2800, loss: 17.091813\n",
      "iter 2900, loss: 15.476560\n",
      "----\n",
      " irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "Allllll\n",
      "edd any further, hear me speak.\n",
      "\n",
      "Allllll\n",
      "rd ar pyak.\n",
      "\n",
      "Alllllrst Citizen:\n",
      "Before me proceed any further, hear me speak.\n",
      "\n",
      "\n",
      "Alllllrst  \n",
      "----\n",
      "iter 3000, loss: 14.014637\n",
      "iter 3100, loss: 12.691477\n",
      "iter 3200, loss: 11.493899\n",
      "iter 3300, loss: 10.409972\n",
      "iter 3400, loss: 9.428900\n",
      "iter 3500, loss: 8.540911\n",
      "iter 3600, loss: 7.737163\n",
      "iter 3700, loss: 7.009653\n",
      "iter 3800, loss: 6.351140\n",
      "iter 3900, loss: 5.755068\n",
      "----\n",
      " irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "Alllllld any further, hen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "Allllll\n",
      "\n",
      "Alldir st Citizen:\n",
      "Before we proceed any further, hear m \n",
      "----\n",
      "iter 4000, loss: 5.215507\n",
      "iter 4100, loss: 4.727090\n",
      "iter 4200, loss: 4.284958\n",
      "iter 4300, loss: 3.884716\n",
      "iter 4400, loss: 3.522385\n",
      "iter 4500, loss: 3.194366\n",
      "iter 4600, loss: 2.897400\n",
      "iter 4700, loss: 2.628539\n",
      "iter 4800, loss: 2.385114\n",
      "iter 4900, loss: 2.164712\n",
      "----\n",
      " irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "Alllllled ar meak.\n",
      "\n",
      "Alllllrs  Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "Allll,ir  Citizen:\n",
      "Before we proceed any further, hea \n",
      "----\n",
      "iter 5000, loss: 1.965146\n",
      "iter 5100, loss: 1.784439\n",
      "iter 5200, loss: 1.620801\n",
      "iter 5300, loss: 1.472613\n",
      "iter 5400, loss: 1.338409\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "n_updates = 0\n",
    "\n",
    "# momentum variables for Adagrad\n",
    "mWex, mWhy = np.zeros_like(Wex), np.zeros_like(Why)\n",
    "mby = np.zeros_like(by) \n",
    "\n",
    "mWf, mWi, mWo, mWc = np.zeros_like(Wf), np.zeros_like(Wi), np.zeros_like(Wo), np.zeros_like(Wc)\n",
    "mbf, mbi, mbo, mbc = np.zeros_like(bf), np.zeros_like(bi), np.zeros_like(bo), np.zeros_like(bc)\n",
    "\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "    \n",
    "while True:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        cprev = np.zeros((hidden_size,1))\n",
    "    p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "    # sample from the model now and then\n",
    "    if n % 1000== 0:\n",
    "        sample_ix = sample((hprev, cprev), inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print ('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, activations, memory = forward(inputs, targets, (hprev, cprev))\n",
    "    gradients = backward(activations)\n",
    "\n",
    "    hprev, cprev = memory\n",
    "    dWex, dWf, dWi, dWo, dWc, dbf, dbi, dbo, dbc, dWhy, dby = gradients\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 100 == 0: print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wf, Wi, Wo, Wc, bf, bi, bo, bc, Wex, Why, by],\n",
    "                                [dWf, dWi, dWo, dWc, dbf, dbi, dbo, dbc, dWex, dWhy, dby],\n",
    "                                [mWf, mWi, mWo, mWc, mbf, mbi, mbo, mbc, mWex, mWhy, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter\n",
    "    n_updates += 1\n",
    "    if n_updates >= max_updates:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0\n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "delta = 0.001\n",
    "\n",
    "hprev = np.zeros((hidden_size, 1))\n",
    "cprev = np.zeros((hidden_size, 1))\n",
    "\n",
    "memory = (hprev, cprev)\n",
    "\n",
    "loss, activations, _ = forward(inputs, targets, memory)\n",
    "gradients = backward(activations, clipping=False)\n",
    "dWex, dWf, dWi, dWo, dWc, dbf, dbi, dbo, dbc, dWhy, dby = gradients\n",
    "\n",
    "for weight, grad, name in zip([Wf, Wi, Wo, Wc, bf, bi, bo, bc, Wex, Why, by], \n",
    "                                [dWf, dWi, dWo, dWc, dbf, dbi, dbo, dbc, dWex , dWhy, dby],\n",
    "                                ['Wf', 'Wi', 'Wo', 'Wc', 'bf', 'bi', 'bo', 'bc', 'Wex', 'Why', 'by']):\n",
    "\n",
    "    str_ = (\"Dimensions dont match between weight and gradient %s and %s.\" % (weight.shape, grad.shape))\n",
    "    assert(weight.shape == grad.shape), str_\n",
    "\n",
    "    print(name)\n",
    "    for i in range(weight.size):\n",
    "      \n",
    "        # evaluate cost at [x + delta] and [x - delta]\n",
    "        w = weight.flat[i]\n",
    "        weight.flat[i] = w + delta\n",
    "        loss_positive, _, _ = forward(inputs, targets, memory)\n",
    "        weight.flat[i] = w - delta\n",
    "        loss_negative, _, _ = forward(inputs, targets, memory)\n",
    "        weight.flat[i] = w  # reset old value for this parameter\n",
    "\n",
    "        grad_analytic = grad.flat[i]\n",
    "        grad_numerical = (loss_positive - loss_negative) / ( 2 * delta )\n",
    "\n",
    "        # compare the relative error between analytical and numerical gradients\n",
    "        rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic)\n",
    "\n",
    "        if rel_error > 0.01:\n",
    "            print ('WARNING %f, %f => %e ' % (grad_numerical, grad_analytic, rel_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(([1,2,3],[2,3,4],[3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [2, 3, 4],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(x[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
