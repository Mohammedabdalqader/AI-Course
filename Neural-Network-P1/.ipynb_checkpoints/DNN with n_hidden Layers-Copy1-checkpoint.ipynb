{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_blobs(n_samples=100000, n_features=12,centers=2,random_state=50)\n",
    "\n",
    "features = data[0]\n",
    "labels   = data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Dataset\n",
    "\n",
    "Here we use function from sklearn Library to split our data into train dataset to train our model on it and test dataset to test our model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train,features_test,labels_train,labels_test = train_test_split(features,labels,test_size=0.30, random_state=1991)\n",
    "labels_train = labels_train.reshape(1,len(labels_train))\n",
    "labels_test = labels_test.reshape(1,len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the shape of train as well as test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your training dataset consist of 70000 samples and 12 features and the shape =  (70000, 12)\n",
      "\n",
      "Your test dataset consist of 30000 samples and 12 features and the shape =  (30000, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"Your training dataset consist of {} samples and {} features and the shape = \".format(features_train.shape[0],features_train.shape[1]), features_train.shape)\n",
    "print(\"\\nYour test dataset consist of {} samples and {} features and the shape = \".format(features_test.shape[0],features_test.shape[1]), features_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to build a neural network with n hidden layers, each containing different number of neurons and output Layer with single neuron\n",
    "Activation Function : Sigmoid\n",
    "\n",
    "Loss Function : Least Square Error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exersise 1.A\n",
    "\n",
    "##### Initialize the weight and bias for each Hidden Layer as well as output Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 12)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "# I will initialize the first Hidden Layer Weight & bias for You\n",
    "\n",
    "n_hidden_layers  = 12\n",
    "\n",
    "# n_hidden_neurons = [n_features,[n_neurons for each hidden_layer], n_output]\n",
    "n_hidden_neurons = [features_train.shape[1],8,16,32,64,32,16,16,8,8,8,16,8,labels_train.shape[0]]  \n",
    "\n",
    "\n",
    "#create dictionary to save all weights & biases in it\n",
    "weight = dict()                         \n",
    "bias   = dict() \n",
    "for i in range(n_hidden_layers+1):\n",
    "    weight[\"weight_h\"+str(i+1)] = np.random.randn(n_hidden_neurons[i+1],n_hidden_neurons[i])\n",
    "    bias[\"bias_h\"+str(i+1)]     = np.random.randn(n_hidden_neurons[i+1],1)\n",
    "        \n",
    "    \n",
    "\n",
    "# you can call for Example weight_h1 from the dictionary as follow :\n",
    "# weight[\"weight_h1\"]\n",
    "\n",
    "print(weight[\"weight_h1\"].shape)\n",
    "\n",
    "# the output_layer weight is \"weight_h7\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation_functions():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # activation functions (Forward)\n",
    "    def sigmoid(self,z):\n",
    "        \n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def softmax(self,z):\n",
    "        shift_z = z - np.max(z)\n",
    "        exps = np.exp(shift_z)\n",
    "        a = exps / np.sum(exps, axis=0,keepdims=True)\n",
    "        return a\n",
    "    \n",
    "    def relu(self,z):\n",
    "        a = np.maximum(z,0)\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    \n",
    "    # activation functions (Backward)\n",
    "    \n",
    "    def sigmoid_backward(self,a):\n",
    "        z  = self.sigmoid(a)\n",
    "        da = z * (1 - z)\n",
    "        return da\n",
    "    \n",
    "    def relu_backward(self,a,z):\n",
    "\n",
    "        dz = np.array(a, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "        # When z <= 0, you should set dz to 0 as well. \n",
    "        dz[z <= 0] = 0\n",
    "    \n",
    "        assert (dz.shape == z.shape)\n",
    "    \n",
    "        return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exersise 1.B\n",
    "\n",
    "Write a Class that contains 4 Functions ( sigmoid , loss function , forward_propagation, backward_propagation)\n",
    "\n",
    "\n",
    "Least square Error = \n",
    "<img src=\"https://latex.codecogs.com/gif.latex?(1/n)&space;*&space;\\sum&space;(labels&space;-&space;pred)^{2}\" title=\"(1/n) * \\sum (labels - pred)^{2}\" />\n",
    "\n",
    "n = number of samples ... in your case it's 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Neural_Network(activation_functions):\n",
    "    \n",
    "    def __init__(self,n_hidden_layers,weight,bias,learning_rate):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.weight          = weight\n",
    "        self.bias            = bias\n",
    "        self.learning_rate   = learning_rate\n",
    "        \n",
    "    \n",
    "    def activation_func_FORWARD(self,act_func,z):\n",
    "        if act_func == \"sigmoid\":\n",
    "            a = self.sigmoid(z)\n",
    "            \n",
    "        if act_func == \"relu\":\n",
    "            a = self.relu(z)\n",
    "            \n",
    "        if act_func == \"softmax\":\n",
    "            a = self.softmax(z)\n",
    "            \n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def activation_func_BACKWARD(self,act_func,a,z):\n",
    "        if act_func == \"sigmoid\":\n",
    "            da = self.sigmoid_backward(a)\n",
    "            \n",
    "        if act_func == \"relu\":\n",
    "            da = self.relu_backward(a,z)\n",
    "            \n",
    "        if act_func == \"softmax\":\n",
    "            a = self.softmax(z)\n",
    "            \n",
    "        return da\n",
    "    \n",
    "    def cross_enthropy(self,labels_train,y_pred):\n",
    "        \n",
    "        loss = -labels_train*np.log(y_pred) - (1-labels_train)*np.log(1-y_pred)\n",
    "        cost = np.mean(loss)\n",
    "        \n",
    "        return cost\n",
    "    def least_square_error(self,labels_train,y_pred):\n",
    "        \n",
    "        return np.mean(np.square(labels_train-y_pred))\n",
    "        \n",
    "        \n",
    "    def forward_propagation(self,features,labels):\n",
    "        ### Here you will implement the feedforward of the Neural_Network\n",
    "        act_func = [\"relu\"]*(n_hidden_layers + 1  )     # you can change the activation function per layer if you want\n",
    "                                                        # but here i will use sigmoid as activation function for all layers\n",
    "        \n",
    "        hidden_layer  = dict()\n",
    "        activation    = dict()\n",
    "        \n",
    "        activation[\"features\"] = features\n",
    "        \n",
    "        \n",
    "        hidden_layer[\"hidden_layer_1\"] = np.add(np.dot(self.weight[\"weight_h1\"],features.T) , self.bias[\"bias_h1\"]) \n",
    "        activation[\"activation_1\"]     = self.activation_func_FORWARD(act_func[0],hidden_layer[\"hidden_layer_1\"])\n",
    "        \n",
    "        for i in range(1,self.n_hidden_layers + 1):\n",
    "            hidden_layer[\"hidden_layer_\" + str(i+1)]  = np.add(np.dot(self.weight[\"weight_h\" + str(i+1)],activation[\"activation_\" + str(i)]) , self.bias[\"bias_h\" + str(i+1)]) \n",
    "            activation[\"activation_\" + str(i+1)]      = self.activation_func_FORWARD(act_func[i],hidden_layer[\"hidden_layer_\" + str(i+1)])    \n",
    "      \n",
    "        \n",
    "        y_pred = list(activation.values())[-1]   \n",
    "        ### compute the loss\n",
    "        \n",
    "        loss = self.least_square_error(labels,y_pred)\n",
    "        \n",
    "        cache = (hidden_layer,activation)\n",
    "        return loss,cache,act_func\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def backward_propagation(self,features,labels,cache,act_func):\n",
    "        \n",
    "        (hidden_layer,activation) = cache\n",
    "        \n",
    "        \n",
    "        hidden_layer = list(hidden_layer.values())\n",
    "        activation   = list(activation.values())\n",
    "        n            = n_hidden_layers + 1\n",
    "        dweight = dict()\n",
    "        dbias   = dict()\n",
    "        for j in range(len(hidden_layer)-1):\n",
    "            if (j==0):\n",
    "                dactivation = -2*(labels - activation[n-j])\n",
    "            else:\n",
    "                dactivation = np.dot(self.weight[\"weight_h\" + str(n-(j-1))].T,dhidden_layer) \n",
    "                \n",
    "            dhidden_layer                 = dactivation * self.activation_func_BACKWARD(act_func[n-(j+1)],activation[n-j],hidden_layer[n-(j+1)])\n",
    "            dweight[\"dweight_h\"+str(n-j)] = np.dot(dhidden_layer,activation[n -(j+1)].T)/features.shape[0] \n",
    "            dbias[\"dbias_h\"+str(n-j)]     = np.mean(dhidden_layer,axis=1).reshape(-1,1)\n",
    "            \n",
    "            #update weight and bias\n",
    "            \n",
    "            self.weight[\"weight_h\" + str(n-j)]  -= self.learning_rate*dweight[\"dweight_h\"+str(n-j)]\n",
    "            self.bias[\"bias_h\" + str(n-j)]      -= self.learning_rate*dbias[\"dbias_h\"+str(n-j)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Deep_Neural_Network(n_hidden_layers,weight,bias,learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vagrant/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:94: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21991605.933329258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vagrant/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: RuntimeWarning: invalid value encountered in less_equal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "# run this cell to train your model :D\n",
    "epochs = 30\n",
    "for epochs in range(epochs):\n",
    "        loss, cache,act_func = model.forward_propagation(features_train,labels_train)\n",
    "        model.backward_propagation(features,labels_train,cache,act_func)\n",
    "        if (epochs % 5 == 0):\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here You can test your model and see tha accuracy :D\n",
    "\n",
    "you just need to run both cells to see how good is your model and Good Luck "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(features_test,labels_test):\n",
    "    \n",
    "    loss, cache = model.forward_propagation(features_test,labels_test)\n",
    "    \n",
    "    y_pred = cache[1][\"activation_7\"]\n",
    "                \n",
    "    loss = model.least_square_error(labels_test,y_pred)\n",
    "    print(loss)\n",
    "    \n",
    "    correct_answer = 0\n",
    "    \n",
    "    for i in range(features_test.shape[0]):\n",
    "        if (labels_test[0][i] == np.round(y_pred[0][i])):\n",
    "            correct_answer +=1\n",
    "            \n",
    "    \n",
    "    accuracy = (correct_answer / features_test.shape[0]) * 100\n",
    "    \n",
    "    return accuracy\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07463498983295852\n",
      "The accuracy of your model is 99.97\n"
     ]
    }
   ],
   "source": [
    "accuracy = test(features_test,labels_test)\n",
    "print(\"The accuracy of your model is {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
