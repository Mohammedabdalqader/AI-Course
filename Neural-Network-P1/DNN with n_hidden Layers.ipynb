{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_blobs(n_samples=100000, n_features=12,centers=2,random_state=98)\n",
    "\n",
    "features = data[0]\n",
    "labels   = data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Dataset\n",
    "\n",
    "Here we use function from sklearn Library to split our data into train dataset to train our model on it and test dataset to test our model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train,features_test,labels_train,labels_test = train_test_split(features,labels,test_size=0.30, random_state=1991)\n",
    "labels_train = labels_train.reshape(1,len(labels_train))\n",
    "labels_test = labels_test.reshape(1,len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the shape of train as well as test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your training dataset consist of 70000 samples and 12 features and the shape =  (70000, 12)\n",
      "\n",
      "Your test dataset consist of 30000 samples and 12 features and the shape =  (30000, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"Your training dataset consist of {} samples and {} features and the shape = \".format(features_train.shape[0],features_train.shape[1]), features_train.shape)\n",
    "print(\"\\nYour test dataset consist of {} samples and {} features and the shape = \".format(features_test.shape[0],features_test.shape[1]), features_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to build a neural network with n hidden layers, each containing different number of neurons and output Layer with single neuron\n",
    "Activation Function : Sigmoid\n",
    "\n",
    "Loss Function : Least Square Error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exersise 1.A\n",
    "\n",
    "##### Initialize the weight and bias for each Hidden Layer as well as output Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 12)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "# I will initialize the first Hidden Layer Weight & bias for You\n",
    "\n",
    "n_hidden_layers  = 6\n",
    "\n",
    "# n_hidden_neurons = [n_features,[n_neurons for each hidden_layer], n_output]\n",
    "n_hidden_neurons = [features_train.shape[1],8,16,32,64,32,16,labels_train.shape[0]]  \n",
    "\n",
    "\n",
    "#create dictionary to save all weights & biases in it\n",
    "weight = dict()                         \n",
    "bias   = dict() \n",
    "for i in range(n_hidden_layers+1):\n",
    "    weight[\"weight_h\"+str(i+1)] = np.random.randn(n_hidden_neurons[i+1],n_hidden_neurons[i])\n",
    "    bias[\"bias_h\"+str(i+1)]     = np.random.randn(n_hidden_neurons[i+1],1)\n",
    "        \n",
    "    \n",
    "\n",
    "# you can call for Example weight_h1 from the dictionary as follow :\n",
    "# weight[\"weight_h1\"]\n",
    "\n",
    "print(weight[\"weight_h1\"].shape)\n",
    "\n",
    "# the output_layer weight is \"weight_h7\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation_functions():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # activation functions (Forward)\n",
    "    def sigmoid(self,z):\n",
    "        \n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def softmax(self,z):\n",
    "        shift_z = z - np.max(z)\n",
    "        exps = np.exp(shift_z)\n",
    "        a = exps / np.sum(exps, axis=0,keepdims=True)\n",
    "        return a\n",
    "    \n",
    "    def relu(self,z):\n",
    "        a = np.maximum(z,0)\n",
    "        return a\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exersise 1.B\n",
    "\n",
    "Write a Class that contains 4 Functions ( sigmoid , loss function , forward_propagation, backward_propagation)\n",
    "\n",
    "\n",
    "Least square Error = \n",
    "<img src=\"https://latex.codecogs.com/gif.latex?(1/n)&space;*&space;\\sum&space;(labels&space;-&space;pred)^{2}\" title=\"(1/n) * \\sum (labels - pred)^{2}\" />\n",
    "\n",
    "n = number of samples ... in your case it's 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Neural_Network(activation_functions):\n",
    "    \n",
    "    def __init__(self,n_hidden_layers,weight,bias,learning_rate):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.weight          = weight\n",
    "        self.bias            = bias\n",
    "        self.learning_rate   = learning_rate\n",
    "        \n",
    "    \n",
    "    def activation_func(self,act_func,z):\n",
    "        if act_func == \"sigmoid\":\n",
    "            a = self.sigmoid(z)\n",
    "            \n",
    "        return a\n",
    "    \n",
    "    def cross_enthropy(self,labels_train,y_pred):\n",
    "        \n",
    "        loss = -labels_train*np.log(y_pred) - (1-labels_train)*np.log(1-y_pred)\n",
    "        cost = np.mean(loss)\n",
    "        \n",
    "        return cost\n",
    "    def least_square_error(self,labels_train,y_pred):\n",
    "        \n",
    "        return np.mean(np.square(labels_train-y_pred))\n",
    "        \n",
    "        \n",
    "    def forward_propagation(self,features,labels):\n",
    "        ### Here you will implement the feedforward of the Neural_Network\n",
    "        act_func = [\"sigmoid\"]*(n_hidden_layers + 1) # you can change the activation function per layer if you want\n",
    "                                                     # but here i will use sigmoid as activation function for all layers\n",
    "        \n",
    "        hidden_layer  = dict()\n",
    "        activation    = dict()\n",
    "        \n",
    "        activation[\"features\"] = features\n",
    "        \n",
    "        \n",
    "        hidden_layer[\"hidden_layer_1\"] = np.add(np.dot(self.weight[\"weight_h1\"],features.T) , self.bias[\"bias_h1\"]) \n",
    "        activation[\"activation_1\"]     = self.activation_func(act_func[0],hidden_layer[\"hidden_layer_1\"])\n",
    "        \n",
    "        for i in range(1,self.n_hidden_layers + 1):\n",
    "            hidden_layer[\"hidden_layer_\" + str(i+1)]  = np.add(np.dot(self.weight[\"weight_h\" + str(i+1)],activation[\"activation_\" + str(i)]) , self.bias[\"bias_h\" + str(i+1)]) \n",
    "            activation[\"activation_\" + str(i+1)]      = self.activation_func(act_func[i],hidden_layer[\"hidden_layer_\" + str(i+1)])    \n",
    "      \n",
    "        \n",
    "        y_pred = list(activation.values())[-1]   \n",
    "        ### compute the loss\n",
    "        \n",
    "        loss = self.least_square_error(labels,y_pred)\n",
    "        \n",
    "        cache = (hidden_layer,activation)\n",
    "        return loss,cache\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def backward_propagation(self,features,labels,cache):\n",
    "        \n",
    "        (hidden_layer,activation) = cache\n",
    "        \n",
    "        \n",
    "        hidden_layer = list(hidden_layer.values())\n",
    "        activation   = list(activation.values())\n",
    "        n            = n_hidden_layers + 1\n",
    "        dweight = dict()\n",
    "        dbias   = dict()\n",
    "        for j in range(len(hidden_layer)-1):\n",
    "            if (j==0):\n",
    "                dactivation = -2*(labels - activation[n-j])\n",
    "            else:\n",
    "                dactivation = np.dot(self.weight[\"weight_h\" + str(n-(j-1))].T,dhidden_layer) \n",
    "                \n",
    "            dhidden_layer                 = dactivation * (activation[n-j]*(1-activation[n-j]))\n",
    "            dweight[\"dweight_h\"+str(n-j)] = np.dot(dhidden_layer,activation[n -(j+1)].T)/features.shape[0] \n",
    "            dbias[\"dbias_h\"+str(n-j)]     = np.mean(dhidden_layer,axis=1).reshape(-1,1)\n",
    "            \n",
    "            #update weight and bias\n",
    "            \n",
    "            self.weight[\"weight_h\" + str(n-j)]  -= self.learning_rate*dweight[\"dweight_h\"+str(n-j)]\n",
    "            self.bias[\"bias_h\" + str(n-j)]      -= self.learning_rate*dbias[\"dbias_h\"+str(n-j)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Deep_Neural_Network(n_hidden_layers,weight,bias,learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'a' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-ae130288afe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-183-25464fa7eb38>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(self, features, labels)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mhidden_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hidden_layer_1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weight_h1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bias_h1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"activation_1\"\u001b[0m\u001b[0;34m]\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_func\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hidden_layer_1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hidden_layers\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-183-25464fa7eb38>\u001b[0m in \u001b[0;36mactivation_func\u001b[0;34m(self, act_func, z)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcross_enthropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'a' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# run this cell to train your model :D\n",
    "epochs = 3000\n",
    "for epochs in range(epochs):\n",
    "        loss, cache= model.forward_propagation(features_train,labels_train)\n",
    "        model.backward_propagation(features,labels_train,cache)\n",
    "        if (epochs % 100 == 0):\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here You can test your model and see tha accuracy :D\n",
    "\n",
    "you just need to run both cells to see how good is your model and Good Luck "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(features_test,labels_test):\n",
    "    \n",
    "    loss, cache = model.forward_propagation(features_test,labels_test)\n",
    "    \n",
    "    y_pred = list(cache[1].values())[-1]  \n",
    "                \n",
    "    loss = model.least_square_error(labels_test,y_pred)\n",
    "    print(loss)\n",
    "    \n",
    "    correct_answer = 0\n",
    "    \n",
    "    for i in range(features_test.shape[0]):\n",
    "        if (labels_test[0][i] == np.round(y_pred[0][i])):\n",
    "            correct_answer +=1\n",
    "            \n",
    "    \n",
    "    accuracy = (correct_answer / features_test.shape[0]) * 100\n",
    "    \n",
    "    return accuracy\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22887700901347693\n",
      "The accuracy of your model is 50.00333333333333\n"
     ]
    }
   ],
   "source": [
    "accuracy = test(features_test,labels_test)\n",
    "print(\"The accuracy of your model is {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
