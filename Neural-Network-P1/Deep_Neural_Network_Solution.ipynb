{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_blobs(n_samples=10000, n_features=4,centers=2,random_state=5)\n",
    "\n",
    "features = data[0]\n",
    "labels   = data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Dataset\n",
    "\n",
    "Here we use function from sklearn Library to split our data into train dataset to train our model on it and test dataset to test our model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train,features_test,labels_train,labels_test = train_test_split(features,labels,test_size=0.30, random_state=1991)\n",
    "labels_train = labels_train.reshape(1,len(labels_train))\n",
    "labels_test = labels_test.reshape(1,len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the shape of train as well as test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your training dataset consist of 7000 samples and 4 features and the shape =  (7000, 4)\n",
      "\n",
      "Your test dataset consist of 3000 samples and 4 features and the shape =  (3000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Your training dataset consist of {} samples and {} features and the shape = \".format(features_train.shape[0],features_train.shape[1]), features_train.shape)\n",
    "print(\"\\nYour test dataset consist of {} samples and {} features and the shape = \".format(features_test.shape[0],features_test.shape[1]), features_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to build a neural network with 2 hidden layers, each containing (8) neurons and output Layer with single neuron\n",
    "Activation Function : Sigmoid\n",
    "\n",
    "Loss Function : Least Square Error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exersise 1.A\n",
    "\n",
    "##### Initialize the weight and bias for each Hidden Layer as well as output Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1991)\n",
    "# I will initialize the first Hidden Layer Weight & bias for You\n",
    "\n",
    "weight_h1 = np.random.rand(32,4)  ## 4 : number of features (number of inputs in the first Hidden Layer)   &&  32 : number of neurons in the first Hidden layer\n",
    "bias_h1   = np.random.rand(32,1)  ## 32 : number of neurons in the first Hidden Layer \n",
    "\n",
    "\n",
    "##### Write Your Code Here ####\n",
    "weight_h2 = np.random.rand(64,32)\n",
    "bias_h2   = np.random.rand(64,1)\n",
    "\n",
    "\n",
    "weight_o3 = np.random.rand(1,64)\n",
    "bias_o3   = np.random.rand(1,1)\n",
    "\n",
    "# then save all weights in one variable and all bias in one variable\n",
    "weight = (weight_h1,weight_h2,weight_o3)\n",
    "bias   = (bias_h1,bias_h2,bias_o3)\n",
    "\n",
    "# you can get weight_h1 for example like this   \n",
    "# weight_h1 = weight[0]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exersise 1.B\n",
    "\n",
    "Write a Class that contains 4 Functions ( sigmoid , loss function , forward_propagation, backward_propagation)\n",
    "\n",
    "\n",
    "Least square Error = \n",
    "<img src=\"https://latex.codecogs.com/gif.latex?(1/n)&space;*&space;\\sum&space;(labels&space;-&space;pred)^{2}\" title=\"(1/n) * \\sum (labels - pred)^{2}\" />\n",
    "\n",
    "n = number of samples ... in your case it's 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Neural_Network():\n",
    "    \n",
    "    def __init__(self,features_train,labels_train,weight,bias,learning_rate):\n",
    "        \n",
    "        self.features_train = features_train\n",
    "        self.labels_train   = labels_train\n",
    "        self.weight         = weight\n",
    "        self.bias           = bias\n",
    "        self.learning_rate  = learning_rate\n",
    "        \n",
    "    def sigmoid(self,z):\n",
    "        \n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def cross_enthropy(self,labels_train,y_pred):\n",
    "        \n",
    "        loss = -labels_train*np.log(y_pred) - (1-labels_train)*np.log(1-y_pred)\n",
    "        cost = np.mean(loss)\n",
    "        \n",
    "        return cost\n",
    "    def least_square_error(self,labels_train,y_pred):\n",
    "        \n",
    "        return np.mean(np.square(labels_train-y_pred))\n",
    "        \n",
    "        \n",
    "    def forward_propagation(self):\n",
    "        ### Here you will implement the feedforward of the Neural_Network\n",
    "        first_hidden_layer_out  = np.add(np.dot(self.weight[0],self.features_train.T) , self.bias[0])\n",
    "        first_activation        = self.sigmoid(first_hidden_layer_out)\n",
    "        ### Write your code here  ###\n",
    "        \n",
    "        second_hidden_layer_out = np.add(np.dot(self.weight[1],first_activation) , self.bias[1])\n",
    "        second_activation       = self.sigmoid(second_hidden_layer_out)\n",
    "        \n",
    "        output_layer            = np.add(np.dot(self.weight[2],second_activation) , self.bias[2])\n",
    "        output_activation       = self.sigmoid(output_layer)\n",
    "        \n",
    "        y_pred = output_activation        \n",
    "        ### compute the loss\n",
    "        \n",
    "        loss = self.least_square_error(self.labels_train,y_pred)\n",
    "        \n",
    "        cache = ((first_hidden_layer_out,second_hidden_layer_out,output_layer),(first_activation,second_activation,y_pred))\n",
    "        return y_pred,loss,cache\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def backward_propagation(self,y_pred,cache):\n",
    "        \n",
    "        dy_pred                  = -2*(self.labels_train - y_pred)\n",
    "        doutput_layer            = dy_pred * (y_pred*(1-y_pred))\n",
    "        dweight_o3               = np.dot(doutput_layer,cache[0][1].T)/self.features_train.shape[0]\n",
    "        dbias_o3                 = np.mean(doutput_layer,axis=1)\n",
    "        \n",
    "        dsecond_activation       = np.dot(self.weight[2].T,cache[0][2])\n",
    "        dsecond_hidden_layer_out = dsecond_activation * (cache[1][1]* (1- cache[1][1])) \n",
    "        dweight_h2               = np.dot(dsecond_hidden_layer_out,cache[1][0].T)/self.features_train.shape[0]\n",
    "        dbias_h2                 = np.mean(dsecond_hidden_layer_out,axis=1)        \n",
    "        \n",
    "        dfirst_activation        = np.dot(self.weight[1].T,cache[0][1])\n",
    "        dfirst_hidden_layer_out = dfirst_activation * (cache[1][0]* (1- cache[1][0])) \n",
    "        dweight_h1               = np.dot(dfirst_hidden_layer_out,self.features_train)/self.features_train.shape[0]\n",
    "        dbias_h1                 = np.mean(dfirst_hidden_layer_out,axis=1) \n",
    "        ##### update your weight and bias (weight_h1,weight_h2,weight_o3......bias_h1,bias_h2,bias_o3)\n",
    "        \n",
    "        weight_h1,weight_h2,weight_o3 = self.weight\n",
    "        bias_h1,bias_h2,bias_o3       = self.bias\n",
    "        weight_h1 -= self.learning_rate * dweight_h1\n",
    "        weight_h2 -= self.learning_rate * dweight_h2\n",
    "        weight_o3 -= self.learning_rate * dweight_o3\n",
    "        \n",
    "        bias_h1   -= self.learning_rate * dbias_h1.reshape(32,1)\n",
    "        bias_h2   -= self.learning_rate * dbias_h2.reshape(64,1)\n",
    "        bias_o3   -= self.learning_rate * dbias_o3\n",
    "        \n",
    "        self.weight = (weight_h1,weight_h2,weight_o3)\n",
    "        self.bias   = (bias_h1,bias_h2,bias_o3)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Deep_Neural_Network(features_train,labels_train,weight,bias,learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5004285714285195\n",
      "0.46631369553840607\n",
      "0.14178700740842895\n"
     ]
    }
   ],
   "source": [
    "# run this cell to train your model :D\n",
    "epochs = 15000\n",
    "for epochs in range(epochs):\n",
    "        y_pred, loss, cache = model.forward_propagation()\n",
    "        model.backward_propagation(y_pred,cache)\n",
    "        if (epochs % 5000 == 0):\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here You can test your model and see tha accuracy :D\n",
    "\n",
    "you just need to run both cells to see how good is your model and Good Luck "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(features_test,labels_test):\n",
    "    \n",
    "    h1  = np.add(np.dot(model.weight[0],features_test.T) , model.bias[0])\n",
    "    a1  = model.sigmoid(h1)\n",
    "        \n",
    "    h2  = np.add(np.dot(model.weight[1],a1) , model.bias[1])\n",
    "    a2  = model.sigmoid(h2)\n",
    "        \n",
    "    out = np.add(np.dot(model.weight[2],a2) , model.bias[2])\n",
    "    a3  = model.sigmoid(out)\n",
    "        \n",
    "    y_pred = a3\n",
    "                \n",
    "    loss = model.least_square_error(labels_test,y_pred)\n",
    "    \n",
    "    correct_answer = 0\n",
    "    \n",
    "    for i in range(features_test.shape[0]):\n",
    "        if (labels_test[0][i] == np.round(y_pred[0][i])):\n",
    "            correct_answer +=1\n",
    "            \n",
    "    \n",
    "    accuracy = (correct_answer / features_test.shape[0]) * 100\n",
    "    \n",
    "    return accuracy\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of your model is 100.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = test(features_test,labels_test)\n",
    "print(\"The accuracy of your model is {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
